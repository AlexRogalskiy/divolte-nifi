version: "3.3"
services:

  # Kafka/Zookeeper container
  kafka:
    image: krisgeus/docker-kafka
    environment:
      LOG_RETENTION_HOURS: 1
      AUTO_CREATE_TOPICS: "true"
      KAFKA_CREATE_TOPICS: divolte:4:1
      ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,INTERNAL://localhost:9093
      LISTENERS: PLAINTEXT://0.0.0.0:9092,INTERNAL://0.0.0.0:9093
      SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT
      INTER_BROKER: INTERNAL
    ports:
      - 9092:9092
      - 2181:2181

  # Divolte container
  divolte:
    image: divolte/divolte-collector
    environment:
      DIVOLTE_KAFKA_BROKER_LIST: kafka:9092
    ports:
      - 8290:8290
    depends_on:
      - kafka

  # NiFi container
  nifi:
    image: apache/nifi:latest
    ports:
      - 8080:8080
    # environment:
    #     NIFI_WEB_HTTP_PORT: 8080
    volumes:
      # - ./nifi_data:/opt/nifi/nifi-current
      - ./hadoop_conf:/etc/hadoop

  ftpd_server:
    image: stilliard/pure-ftpd
    ports:
      - "21:21"
      - "30000-30009:30000-30009"
    volumes: # remember to replace /folder_on_disk/ with the path to where you want to store the files on the host machine
      - "./ftp_data/data:/home/username/"
      - "./ftp_data/passwd:/etc/pure-ftpd/passwd"
    environment:
      PUBLICHOST: "ftpd_server"
      FTP_USER_NAME: username
      FTP_USER_PASS: mypass
      FTP_USER_HOME: /home/username

  # HDFS namenode
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  # HDFS datanode
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env

volumes:
  hadoop_namenode:
  hadoop_datanode: